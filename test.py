import pandas as pd
import time
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
import os

# ì›¹ë“œë¼ì´ë²„ ì„¤ì •
def setting():
    global driver
    options = webdriver.ChromeOptions()
    options.add_experimental_option("detach", True)
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get("https://tossinvest.com/stocks/A005930/community")
    time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

# ì •ë ¬ì„ ìµœì‹ ìˆœìœ¼ë¡œ ë³€ê²½
def sorting():
    try:
        sort_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, 'section._1m9brd0 > button'))
        )
        if sort_button.text.strip() == "ì¸ê¸°ìˆœ":
            sort_button.click()
            time.sleep(2)  # ì •ë ¬ ë³€ê²½ ë°˜ì˜ ëŒ€ê¸°
        print("ğŸ”¹ ì •ë ¬ ìƒíƒœ: ìµœì‹ ìˆœ")
    except Exception as e:
        print(f"âš  ì •ë ¬ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")

# ë°ì´í„° ì €ì¥ìš© ë³€ìˆ˜
collected_data_index = set()
collected_ids = set()
collected_comments = []

# ê¸°ì¡´ ID ë¡œë“œ
def load_existing_ids(file_path="test_comments.csv"):
    global collected_ids
    if os.path.exists(file_path):
        df = pd.read_csv(file_path, encoding="utf-8-sig")

        collected_ids = set(df["post_id"].dropna().astype(str))
        print(f"ğŸ”¹ ê¸°ì¡´ post_id ê°œìˆ˜: {len(collected_ids)}ê°œ ë¡œë“œ ì™„ë£Œ!")
    else:
        print("ğŸ”¹ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ (ìƒˆë¡œ ìˆ˜ì§‘ ì‹œì‘)")


# ëŒ“ê¸€ ìˆ˜ì§‘
def collect_comments():
    global collected_comments, collected_data_index

    try:
        comment_elements = driver.find_elements(By.CSS_SELECTOR, '#stock-content [data-index]')
        print(f'ğŸ” comment_elementsì˜ ê¸¸ì´: {len(comment_elements)}')

        new_comments = 0  

        for comment in comment_elements:
            try:
                data_index = int(comment.get_attribute("data-index"))

                if data_index < 2:
                    continue
                if data_index in collected_data_index:
                    continue

                # âœ… article.commentì—ì„œ post_id ê°€ì ¸ì˜¤ê¸°
                try:
                    article = comment.find_element(By.CSS_SELECTOR, "article.comment")
                except:
                    continue

                post_id = article.get_attribute("data-post-anchor-id")

                # âœ… post_idê°€ None ë˜ëŠ” ë¹ˆ ê°’ì´ë©´ ê±´ë„ˆë›°ê¸°
                if not post_id:
                    continue

                # âœ… ì¤‘ë³µ ë°©ì§€ (ê¸°ì¡´ post_idê°€ ìˆë‹¤ë©´ ìŠ¤í‚µ)
                if post_id in collected_ids:
                    print(f"âš  ì¤‘ë³µëœ post_id ë°œê²¬, ìŠ¤í‚µ: {post_id}")
                    continue

                # âœ… ì œëª©ê³¼ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                try:
                    title_element = article.find_element(By.CSS_SELECTOR, "span._1sihfl60")
                    comment_title = title_element.text.strip()
                except:
                    comment_title = ""

                try:
                    text_element = article.find_element(By.CSS_SELECTOR, "span._60z0ev1")
                    comment_text = text_element.text.strip()
                except:
                    comment_text = ""

                # âœ… ì‹œê°„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                try:
                    time_element = article.find_element(By.CSS_SELECTOR, "time")
                    timestamp = time_element.get_attribute('datetime')
                except:
                    timestamp = ""

                # âœ… ì¤‘ë³µ ë°©ì§€ ëª©ë¡ì— ì¶”ê°€ (ë‹¤ìŒ ìŠ¤í¬ë¡¤ì—ì„œë„ ì¤‘ë³µ ë°©ì§€)
                collected_ids.add(post_id)
                collected_data_index.add(data_index)
                collected_comments.append((post_id, data_index, comment_title, comment_text, timestamp))

                new_comments += 1
                print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: post_id={post_id}, index={data_index}")

            except Exception as e:
                print(f"âš  ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

        print(f"âœ… í˜„ì¬ í™”ë©´ì—ì„œ {new_comments}ê°œ ëŒ“ê¸€ ì¶”ê°€ ìˆ˜ì§‘ ì™„ë£Œ!")
        return new_comments
    except Exception as e:
        print(f"âš  ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0


# # ìŠ¤í¬ë¡¤ ë° ë°˜ë³µ ìˆ˜ì§‘
def scroll_and_collect(repeat_count):
    global collected_data_index

    print("ğŸ”¹ ì´ˆê¸° ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘")
    collect_comments()

    for i in range(repeat_count):
        prev_max_index = max(collected_data_index) if collected_data_index else 0
        print(f"ğŸ”½ {i+1}/{repeat_count} ë²ˆì§¸ ìŠ¤í¬ë¡¤ ì§„í–‰... (í˜„ì¬ ìµœëŒ€ data-index: {prev_max_index})")

        # ìŠ¤í¬ë¡¤ ì‹¤í–‰
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        # ìƒˆë¡œìš´ ëŒ“ê¸€ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        for _ in range(5):  # 5ì´ˆ ë™ì•ˆ ë°˜ë³µ í™•ì¸
            new_comments = collect_comments()
            if new_comments > 0:
                break
            time.sleep(1)

        if new_comments == 0:
            print("ğŸš¨ ìƒˆë¡œìš´ ëŒ“ê¸€ì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ. ìŠ¤í¬ë¡¤ ì¤‘ë‹¨.")
            break  # ë” ì´ìƒ ìƒˆë¡œìš´ ëŒ“ê¸€ì´ ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤ ì¤‘ë‹¨

    print("âœ… ìŠ¤í¬ë¡¤ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ!")


# ë°ì´í„° ì €ì¥
def save_comments(file_path="test_comments.csv"):
    if not collected_comments:
        print("âš  ì €ì¥í•  ìƒˆ ëŒ“ê¸€ ì—†ìŒ")
        return

    df_new = pd.DataFrame(collected_comments, columns=["post_id", "data_index", "title", "comment", "timestamp"])

    # âœ… ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ í›„ ì¤‘ë³µ ì œê±°
    if os.path.exists(file_path):
        df_existing = pd.read_csv(file_path, encoding="utf-8-sig")
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        df_combined = df_combined.drop_duplicates(subset=["post_id"], keep="first")  # âœ… ì¤‘ë³µ ì œê±°
        df_combined.to_csv(file_path, index=False, encoding="utf-8-sig")
    else:
        df_new.to_csv(file_path, index=False, encoding="utf-8-sig")

    print(f"âœ… ì €ì¥ ì™„ë£Œ! (ìµœì¢… ì €ì¥ëœ ëŒ“ê¸€ ê°œìˆ˜: {len(df_new)}ê°œ)")
    collected_comments.clear()

# ì‹¤í–‰ í•¨ìˆ˜
def run_scraper(repeat_count=5, file_path="test_comments.csv"):
    load_existing_ids(file_path)
    scroll_and_collect(repeat_count)
    save_comments(file_path)

# # ì‹¤í–‰
setting()
sorting()
run_scraper(repeat_count=5, file_path="test_comments.csv")


