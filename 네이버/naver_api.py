import requests
import pandas as pd
import os
from datetime import datetime, timedelta

# ì €ì¥ í´ë” ì„¤ì •
SAVE_PATH = "./data/"
os.makedirs(SAVE_PATH, exist_ok=True)

# ë„¤ì´ë²„ ì¦ê¶Œ ì¢…ëª©í† ë¡  API ê¸°ë³¸ URL
DISCUSS_URL = "https://m.stock.naver.com/front-api/discuss"

# HTTP ìš”ì²­ í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
    "Accept": "application/json"
}

# í¬ë¡¤ë§í•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë° discussionType(global/local) êµ¬ë¶„
stocks = {
    "NVDA.O": {"name": "ì—”ë¹„ë””ì•„", "type": "globalStock"},
    "000660": {"name": "SKí•˜ì´ë‹‰ìŠ¤", "type": "localStock"},
    "TSLA.O": {"name": "í…ŒìŠ¬ë¼", "type": "globalStock"},
    "005930": {"name": "ì‚¼ì„±ì „ì", "type": "localStock"},
    "068270": {"name": "ì…€íŠ¸ë¦¬ì˜¨", "type": "localStock"},
    
}

def load_existing_post_ids(stock_code):
    """ ê¸°ì¡´ CSV íŒŒì¼ì—ì„œ ì €ì¥ëœ post_id ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ """
    stock_name = stocks[stock_code]["name"]
    file_path = os.path.join(SAVE_PATH, f"naver_{stock_name}.csv")

    if os.path.exists(file_path):
        df_existing = pd.read_csv(file_path, usecols=["post_id"], dtype={"post_id": str}, encoding="utf-8-sig")
        return set(df_existing["post_id"])  # ì¤‘ë³µ ê²€ì‚¬ ìµœì í™”
    return set()

def request_discussions(stock_code, discussion_type, rsno=None):
    """ API ìš”ì²­ì„ ë³´ë‚´ê³  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    params = {
        "discussionType": discussion_type,
        "itemCode": stock_code,
        "size": 20,  # ìµœì‹  20ê°œ ëŒ“ê¸€ ê°€ì ¸ì˜´
    }
    if rsno:
        params["rsno"] = rsno  # ì´ì „ ëŒ“ê¸€ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ìš”ì²­

    response = requests.get(DISCUSS_URL, params=params, headers=HEADERS)
    
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return []

    data = response.json()
    if not data.get("isSuccess"):
        print("âŒ API ì‘ë‹µ ì˜¤ë¥˜ ë°œìƒ")
        return []

    return data.get("result", [])

def process_discussions(results, start_datetime, end_datetime, existing_post_ids):
    """ API ì‘ë‹µ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
    discussions = []
    last_rsno = None
    stop_collection = False  # âœ… ì¤‘ì§€ ì¡°ê±´ì„ í™•ì¸í•˜ëŠ” ë³€ìˆ˜

    for post in results:
        post_date = datetime.strptime(post.get("date"), "%Y-%m-%d %H:%M:%S.%f")
        post_id = str(post.get("discussionId"))

        # âœ… ì§€ì •ëœ ì‹œê°„ ì´ì „ ë°ì´í„°ê°€ ë‚˜ì˜¤ë©´ ì¤‘ì§€ í”Œë˜ê·¸ ì„¤ì •
        if post_date < start_datetime:
            print(f"â¹ï¸ ì§€ì •ëœ ì‹œê°„ ì´ì „ ë°ì´í„° ë°œê²¬, ìˆ˜ì§‘ ì¤‘ì§€ ({post_date})")
            stop_collection = True
            break

        # âœ… ì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ë‚˜ì˜¤ë©´ ì¤‘ì§€ í”Œë˜ê·¸ ì„¤ì •
        if post_id in existing_post_ids:
            print(f"ğŸ”´ ê¸°ì¡´ ë°ì´í„°({post_id}) ë°œê²¬, ìˆ˜ì§‘ ì¤‘ì§€")
            stop_collection = True
            break

        # âœ… ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„° ì¶”ê°€ (ì¦‰ì‹œ ì €ì¥ X)
        discussions.append({
            "platform": "ë„¤ì´ë²„ì¦ê¶Œ",
            "stock_name": stocks[post["itemCode"]]["name"],
            "post_id": post_id,
            "title": post.get("title"),
            "content": post.get("contents"),
            "timestamp": post.get("date")
        })

        last_rsno = post.get("rsno")  # ê°€ì¥ ì˜¤ë˜ëœ ê¸€ì˜ rsno ì €ì¥

    return discussions, last_rsno, stop_collection

def save_to_csv(stock_code, discussions):
    """ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•œêº¼ë²ˆì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ """
    stock_name = stocks[stock_code]["name"]
    file_path = os.path.join(SAVE_PATH, f"naver_{stock_name}.csv")

    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(discussions, columns=["platform", "stock_name", "post_id", "title", "content", "timestamp"])
    df = df.sort_values(by="timestamp", ascending=True)

    if os.path.exists(file_path):
        existing_df = pd.read_csv(file_path, dtype={"post_id": str}, encoding="utf-8-sig")
        combined_df = pd.concat([existing_df, df]).drop_duplicates(subset=["post_id"]).sort_values(by="timestamp")
    else:
        combined_df = df

    combined_df.to_csv(file_path, index=False, encoding="utf-8-sig")
    print(f"âœ… {stock_name} ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path}")

def fetch_and_save_discussions(stock_code, discussion_type, start_datetime, end_datetime):
    """ íŠ¹ì • ì¢…ëª©ì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜ """
    existing_post_ids = load_existing_post_ids(stock_code)
    rsno = None  
    all_discussions = []  # âœ… ë°ì´í„°ë¥¼ ëª¨ì•„ë‘ëŠ” ë¦¬ìŠ¤íŠ¸

    while True:
        results = request_discussions(stock_code, discussion_type, rsno)
        if not results:
            print(f"âœ… ë” ì´ìƒ ê°€ì ¸ì˜¬ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ({stock_code})")
            break

        discussions, rsno, stop_collection = process_discussions(results, start_datetime, end_datetime, existing_post_ids)

        if discussions:
            all_discussions.extend(discussions)  # âœ… ë¦¬ìŠ¤íŠ¸ì— ë°ì´í„° ì¶”ê°€

        if stop_collection:  # âœ… ì¤‘ì§€ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ ì¦‰ì‹œ ì¢…ë£Œ
            break

    if all_discussions:
        save_to_csv(stock_code, all_discussions)  # âœ… í•œ ë²ˆì— ì €ì¥

def collect_and_save_all_discussions(hours_ago):
    """ ì „ì²´ ì¢…ëª©ì— ëŒ€í•´ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜ """
    start_datetime = datetime.now() - timedelta(hours=hours_ago)
    end_datetime = datetime.now()

    for stock_code, info in stocks.items():
        print(f"ğŸ” {info['name']}({stock_code}) ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        fetch_and_save_discussions(stock_code, info["type"], start_datetime, end_datetime)

# âœ… ì‹¤í–‰
collect_and_save_all_discussions(hours_ago=72)
