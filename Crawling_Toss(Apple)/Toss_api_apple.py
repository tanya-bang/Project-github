import requests
import time
import csv
from datetime import datetime
from datetime import datetime, date


# ìš”ì²­í•  URL
url = "https://wts-cert-api.tossinvest.com/api/v3/comments"

# ì¢…ëª©ëª… ë³€ìˆ˜í™”
stock_name = "ì• í”Œ"

# ìš”ì²­ í—¤ë” (ì¿ í‚¤ì™€ XSRF í† í°ì€ ê·¸ë•Œê·¸ë•Œë§ˆë‹¤ ë³€ê²½ í•„ìš”)
headers = {
    "accept": "application/json",
    "accept-encoding": "gzip, deflate, br, zstd",
    "accept-language": "ko,en-US;q=0.9,en;q=0.8",
    "app-version": "2025-03-19 16:20:12",
    "browser-tab-id": "browser-tab-de1a0d07c24449f2ba4fdad4975d7d62",
    "content-type": "application/json",
    "origin": "https://tossinvest.com",
    "referer": "https://tossinvest.com/stocks/US19801212001/community",
    "sec-ch-ua": '"Chromium";v="134", "Not:A-Brand";v="24", "Google Chrome";v="134"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-site",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36",
    "x-xsrf-token": "359f6ce0-5793-4a04-95d7-02cf9881919a",
    "cookie": "deviceId=WTS-7b52e417e49e4530b9945b6f0b34c6d0; _ga=GA1.1.757933857.1741746259; x-toss-distribution-id=53; XSRF-TOKEN=359f6ce0-5793-4a04-95d7-02cf9881919a; _browserId=d50c413bcef94076850bbb260218dca3; BTK=Tt1v1mRRkk6lzkm3tmnA5EN2yr1ePUyRI3b6k75cbQ4=; SESSION=OTBiN2Y1OTAtMGMzYi00OTMzLThkMTQtNDVhZmI0NmIwZjAw; _ga_T5907TQ00C=GS1.1.1743037968.13.1.1743037993.0.0.0"
}


# ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„ (YYYY-MM-DD)
start_date = "2023-03-27"
end_date = date.today().strftime("%Y-%m-%d")  # ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê³ ì •

# íŒŒì¼ëª…
csv_filename = "apple_comments.csv"
cache_filename = "cache_apple_comments.csv"

# ì• í”Œ
data = {
    "subjectId": "US19801212001",
    "subjectType": "STOCK",
    "commentSortType": "RECENT"
}

# ì „ì²´ ëŒ“ê¸€ ì €ì¥ ë¦¬ìŠ¤íŠ¸
all_comments = []
previous_count = 0

# ë‚ ì§œ ë¹„êµ í•¨ìˆ˜
def is_within_date_range(comment_date):
    comment_datetime = datetime.strptime(comment_date[:10], "%Y-%m-%d")
    start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
    end_datetime = datetime.strptime(end_date, "%Y-%m-%d")
    return start_datetime <= comment_datetime <= end_datetime

# ìºì‹œ ì €ì¥ í•¨ìˆ˜
def save_cache(file_path, data):
    with open(file_path, "w", newline="", encoding="utf-8") as cache_file:
        writer = csv.writer(cache_file)
        writer.writerow(["Comment ID", "Message", "Updated At", "Nickname", "Platform", "Stock Name"])
        writer.writerows(data)
    print(f"ğŸ“ ì„ì‹œ ìºì‹œ ì €ì¥: {len(data)}ê°œ â†’ {file_path}")
last_seen_id = None  # ë§ˆì§€ë§‰ ëŒ“ê¸€ ID ì €ì¥

page_count = 0       # í˜ì´ì§€ ìˆ˜ ì¹´ìš´í„° ì¶”ê°€

while True:
    response = requests.post(url, json=data, headers=headers)

    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}, {response.text}")
        break

    json_data = response.json()
    comments = json_data.get("result", {}).get("comments", {}).get("body", [])
    has_next = json_data.get("result", {}).get("comments", {}).get("hasNext", False)

    if not comments:
        print("\nğŸš€ ë” ì´ìƒ ëŒ“ê¸€ì´ ì—†ìœ¼ë¯€ë¡œ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    # ë§ˆì§€ë§‰ ëŒ“ê¸€ ë‚ ì§œê°€ ìˆ˜ì§‘ ë²”ìœ„ ë°–ì´ë©´ ì¢…ë£Œ
    last_comment_date = comments[-1]["updatedAt"]
    if not is_within_date_range(last_comment_date):
        print(f"\nğŸ›‘ ë§ˆì§€ë§‰ ëŒ“ê¸€ ë‚ ì§œ({last_comment_date})ê°€ ë²”ìœ„ ë°–ì´ë¯€ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break
    
    page_count += 1  # ğŸ”¹ í˜ì´ì§€ ìˆ˜ ì¦ê°€

    # ì¤‘ê°„ ìš”ì•½ ë¡œê·¸ ì¶œë ¥ (10ì˜ ë°°ìˆ˜ì¼ ë•Œë§Œ)
    if page_count % 10 == 0:
        print(f"ğŸŒ€ {page_count}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... ëˆ„ì  ëŒ“ê¸€ ìˆ˜: {len(all_comments)}")

    # ê°€ì¥ ë§ˆì§€ë§‰ ëŒ“ê¸€ ID ì¶”ì¶œ
    current_last_id = comments[-1]["id"]

    # ì´ì „ì— ë´¤ë˜ IDì™€ ë™ì¼í•˜ë©´ ì¢…ë£Œ
    if current_last_id == last_seen_id:
        print("\nğŸš€ ë” ì´ìƒ ìƒˆë¡œìš´ ëŒ“ê¸€ì´ ì—†ì–´ ìˆ˜ì§‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    last_seen_id = current_last_id

    new_comments = 0
    for comment in comments:
        comment_date = comment["updatedAt"]
        if is_within_date_range(comment_date):
            all_comments.append([
                comment["id"],
                comment["message"],
                comment_date,
                comment["author"]["nickname"],
                "toss",
                stock_name
            ])
            new_comments += 1

    current_count = len(all_comments)
    #print(f"âœ… ìˆ˜ì§‘ëœ ëŒ“ê¸€ ê°œìˆ˜: {current_count} (ì´ë²ˆ í˜ì´ì§€ ì‹ ê·œ: {new_comments})")

    if current_count - previous_count >= 100:
        save_cache(cache_filename, all_comments)
        previous_count = current_count

    data["commentId"] = current_last_id
    time.sleep(1)

# ìµœì¢… ì €ì¥
with open(csv_filename, "w", newline="", encoding="utf-8") as final_file:
    writer = csv.writer(final_file)
    writer.writerow(["Comment ID", "Message", "Updated At", "Nickname", "Platform", "Stock Name"])
    writer.writerows(all_comments)

print(f"\nğŸ‰ ìµœì¢…ì ìœ¼ë¡œ {len(all_comments)}ê°œì˜ ëŒ“ê¸€ì´ {csv_filename} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
